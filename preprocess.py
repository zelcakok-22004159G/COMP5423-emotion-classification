'''
    Filename: preprocess.py
    Usage: Standalone file to pre-process the training and validation datasets
'''
import pandas as pd
import numpy as np
import nltk
import shutil
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from json import dumps
from tqdm import tqdm

from libs.data_preprocessor import DataProcessor

nltk.download('stopwords')
nltk.download('punkt')

dp = DataProcessor()

# Load the stopwords generated by the stopwords.py
custom_words = pd.read_csv(
    "data/stopwords.txt", header=None).to_numpy().flatten().tolist()
ignore_words = np.array(stopwords.words('english') + custom_words)


def process_training_file(raw, updated):
    # Measuring the trimming processing
    stats = {"max": 0, "min": None}
    df = pd.read_csv(raw, header=None, names=["Sentence", "Emotion"], sep=";")

    for i in tqdm(range(0, len(df)), total=len(df)):
        # Get the sentence
        string = df.at[i, 'Sentence']
        # Lemmatization and Stmming
        string = dp.process_line(string)
        # Remove stopwords
        tokens = np.setdiff1d(np.array(word_tokenize(string)), ignore_words)
        # Create string from a list of tokens
        string = ' '.join(tokens)
        length = len(string)
        if not length:
            # Drop the sentence if length = 0
            df.drop(i, inplace=True)
        else:
            stats["max"] = max(stats["max"], length)
            stats["min"] = min(stats["min"], len(
                tokens)) if stats["min"] is not None else length
            # Update the line
            df.at[i, 'Sentence'] = string

    df.to_csv(updated, index=False, header=None, sep=";")
    return stats

# Reset the trimmed files
shutil.copyfile("data/train_data.txt", "data/train_data_trimmed.txt")
shutil.copyfile("data/val_data.txt", "data/val_data_trimmed.txt")

# Trim the training dataset
stats = process_training_file("data/train_data_trimmed.txt",
                              "data/train_data_trimmed.txt")
print("Training", dumps(stats, indent=4))

# Trim the validation dataset
stats = process_training_file("data/val_data_trimmed.txt",
                              "data/val_data_trimmed.txt")
print("Validate", dumps(stats, indent=4))
